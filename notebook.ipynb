{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2S1xz4VkWKWk"
   },
   "source": [
    "IMPORTANT! Before beginning any lab assignment, be sure to **make your own copy** of the notebook and name it \"lastname - Lab 4\" or something similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WppFQvdYNB0p"
   },
   "source": [
    "# Lab 4: Predictive Analytics in Python (Part 2)\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this lab, we will continue working with the churn dataset from Lab 3. The goal is to finish preparing the dataset for modeling (and then - visualize it!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAHRdBjQNidG"
   },
   "source": [
    "### The Scenario: Customer Churn in Telecommunications ðŸ“ž (Continued)\n",
    "\n",
    "In case you need a reminder from Lab 3:\n",
    "* You're a data analyst contracted by a telecommunications company that is experiencing a high rate of customer churn.\n",
    "* The company has noticed a recent decline in active subscriptions.\n",
    "* It's your job to analyze this customer churn, identify trends, and recommend strategies to improve customer retention.\n",
    "\n",
    "In the previous activity, we learned that **we had outliers**n (at least one); on the other hand, we also learned there were **missing values**, for less than a percent of the `TotalCharges` data. Now that we've had a chance to reflect on our understanding of the business and data - and completed our DQP (Data Quality Plan) - we can move on to the Data Preparation stage.\n",
    "\n",
    "**Your goal:** Prepare the dataset by handling data quality issues, engineering relevant features, and implementing initial predictive models to classify whether a customer is likely to churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qg0ErzS1YetE"
   },
   "source": [
    "Before we start, we should load the dataset and refresh ourselves on the details. Recall the dataset contains various features about customer's account information and service usage patterns. The target variable `Churn`indicates whether a customer has churned (i.e., left the service) or not.\n",
    "\n",
    "- `CustomerID`: The unique ID of each customer\n",
    "- `Tenure`: The time for which a customer has been using the service.\n",
    "- `PhoneService`: Whether a customer has a landline phone service along with the internet service.\n",
    "- `Contract`: The type of contract a customer has chosen.\n",
    "- `PaperlessBilling`: Whether a customer has opted for paperless billing.\n",
    "- `PaymentMethod`: Specifies the method by which bills are paid.\n",
    "- `MonthlyCharges`: Specifies the money paid by a customer each month.\n",
    "- `TotalCharges`: The total money paid by the customer to the company.\n",
    "- `Churn`: This is the target variable which specifies if a customer has churned or not.\n",
    "\n",
    "Let's get started by loading the dataset into a Polars DataFrame. If you haven't already downloaded the dataset, the code below will do so for you.  Its keeps a local copy in case you need to restart the notebook.\n",
    "\n",
    "**NOTE**: We are also creating a Pandas version of the Polars DataFrame for compatibility with Seaborn, which we will use for visualization later in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "\n",
    "# Set the URL and local filename\n",
    "url = \"https://raw.githubusercontent.com/JuanCab/csis446_lab04/refs/heads/main/data/churn_data.csv\"\n",
    "local_filename = \"churn_data.csv\"\n",
    "\n",
    "# Check if the file already exists to avoid re-downloading\n",
    "if not Path(local_filename).is_file():\n",
    "    print(f\"Downloading dataset from {url}...\")\n",
    "    # Download the dataset using requests library\n",
    "    r = requests.get(url)\n",
    "    # Check if the request was successful, if not, raise an error\n",
    "    r.raise_for_status()\n",
    "\n",
    "    # Save the content to a local file\n",
    "    with open(local_filename, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "        print(f\"Dataset downloaded and saved as '{local_filename}'.\")\n",
    "\n",
    "# Load the dataset from the local file into a Polars DataFrame\n",
    "churn_df = pl.read_csv(local_filename)\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify successful loading\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remind ourselves we can review some of the basic information about the dataset after loading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the description/statistics of the dataset\n",
    "churn_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built lists of categorical and continuous features in the last lab.  Those will be useful again here.  Let's recreate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_types(dataframe):\n",
    "    \"\"\"Returns two lists: categorical and continuous column names.\"\"\"\n",
    "    # Lists to track column types\n",
    "    categorical_cols = []\n",
    "    continuous_cols = []\n",
    "\n",
    "    for column in dataframe.columns:\n",
    "        # .dtype is the data type of the column\n",
    "        # pl.Float64 and pl.Int64 are Polars data types for float and integer\n",
    "        if dataframe[column].dtype in (pl.Float64, pl.Int64):\n",
    "            continuous_cols.append(column)\n",
    "        else:\n",
    "            categorical_cols.append(column)\n",
    "    return categorical_cols, continuous_cols\n",
    "\n",
    "categorical_cols, continuous_cols = column_types(churn_df)\n",
    "print(f\"{categorical_cols=}\")\n",
    "print(f\"{continuous_cols=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Data Preparation\n",
    "\n",
    "Before we build any models, we need to ensure our data is clean and reliable.\n",
    "\n",
    "In Lab 3, you should have built a draft Data Quality Plan (DQP) in the previous lab. It should have looked something like this:\n",
    "\n",
    "| Feature         | Issue Identified                  | Recommended Action                |\n",
    "|-----------------|----------------------------------|-----------------------------------|\n",
    "| `CustomerID`      | High cardinality (unique values) | Exclude from analysis             |\n",
    "| `tenure`          | Bimodal distribution (new vs long-term customers) | Consider binning or segmentation |\n",
    "| `PhoneService`    | Binary categorical (cardinality 2) | Eventually convert to Boolean type        |\n",
    "| `Contract`        | Categorical variable (low cardinality) | None |\n",
    "| `PaperlessBilling` | Binary categorical (cardinality 2) | Eventually convert to Boolean type        |\n",
    "| `PaymentMethod`   | Categorical variable (low cardinality) | None |\n",
    "| `MonthlyCharges`  | 14 outliers found | Investigate outliers, consider keeping or adjusting|\n",
    "| `TotalCharges`    | Missing values (11 out of 7043, 0.16%), also 19 outliers | Remove rows or impute with mean/median.  Review outlier handling. |\n",
    "| `Churn`           | Target variable (cardinality 2) | Eventually convert to Boolean type        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Addressing Cardinality Issues\n",
    "\n",
    "We already noted that `customerID` has high cardinality (unique values for each customer), which means it is not useful for modeling.  Let's drop this feature from the dataset.  In polars, this can be done using the `drop` method, just list the column name to be dropped as a string (or list of strings for multiple columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Drop the customerID column as it has high cardinality\n",
    "\n",
    "# Drop this column from the categorical columns list as well\n",
    "categorical_cols.remove('customerID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't mention this in class, but for those Boolean features, we can consider converting them to actual Boolean True/False types for better performance during modeling (since most modelling algorithms only work with numeric data).  Let's do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a cardinalities dictionary using a dictionary comprehension\n",
    "cardinalities_comp = { col: churn_df[col].n_unique() for col in categorical_cols }\n",
    "\n",
    "# Loop through dictionary and convert every feature in the polars dataframe\n",
    "# with a cardinality of 2 to Boolean type mapping \"Yes\"/\"No\" values into True/False\n",
    "for col, card in cardinalities_comp.items():\n",
    "    if card == 2:\n",
    "        churn_df = churn_df.with_columns(\n",
    "            pl.when(pl.col(col) == \"Yes\")\n",
    "            .then(True).otherwise(False)\n",
    "            .alias(f\"{col}_Bool\")\n",
    "        )\n",
    "\n",
    "print(\"\\nUpdated DataFrame:\")\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain what you think then `.when`, `.then`, and `.otherwise` methods are doing in this context.  Write your explanation below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "WRITE DOWN YOUR ANSWERS HERE.  KEEP THEM BETWEEN THE TRIPLE BACKTICKS TO HAVE THEM BE FORMATTED AS FIXED-WIDTH TEXT, WHICH MAKES THEM STAND OUT FOR GRADING.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Addressing outliers\n",
    "\n",
    "In Lab 3, we identified some outliers in the `MonthlyCharges` and `TotalCharges` features during our data exploration using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a slightly modified code from Lab 3 used to identify potential \n",
    "# outliers using the IQR method. It stores the lower and upper bounds for each\n",
    "# continuous column in dictionaries for later use.\n",
    "\n",
    "# Initialize dictionaries to store lower and upper bounds\n",
    "lower_bounds = {}\n",
    "upper_bounds = {}\n",
    "\n",
    "for col in continuous_cols:\n",
    "    print(f\"\\nAnalyzing column: {col}\")\n",
    "\n",
    "    # Define Q1, Q3, IQR, lower_bound, upper_bound values for 'col'\n",
    "    Q1 = churn_df[col].quantile(0.25)\n",
    "    Q3 = churn_df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    # Store bounds in dictionaries\n",
    "    lower_bounds[col] = lower_bound\n",
    "    upper_bounds[col] = upper_bound\n",
    "\n",
    "    # Print IQR and bounds\n",
    "    print(f\"IQR for {col}:\", IQR)\n",
    "    print(f\"Lower/Upper Bounds for {col}:\", \n",
    "          lower_bounds[col], upper_bounds[col])\n",
    "\n",
    "    # Check against the dataframe to find potential outliers\n",
    "    outliers = churn_df.filter((pl.col(col) < lower_bounds[col]) | \n",
    "                               (pl.col(col) > upper_bounds[col]))\n",
    "\n",
    "    print(f\"Number of Potential Outliers for {col}: {outliers.height}\")\n",
    "\n",
    "    # Show the values of the outliers\n",
    "    print(f\"\\nPotential Outliers for {col}:\")\n",
    "    print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the outliers visually using box plots.  We'll use Seaborn for this, which works with Pandas DataFrames, so we'll use the Pandas version of our data.  Examine the plots and describe what you see below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas version of the Polars DataFrame for compatibility\n",
    "# with Seaborn\n",
    "churn_pd_df = churn_df.to_pandas()\n",
    "\n",
    "# Make box plots to visualize outliers in MonthlyCharges and TotalCharges\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the matplotlib figure with two subplots\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Split the figure into 2 rows and 1 column, and place next plot in \n",
    "# the first subplot\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.boxplot(x=churn_pd_df['MonthlyCharges'], color='skyblue', orient='h')\n",
    "plt.title('Box Plot of MonthlyCharges')\n",
    "# TO DO: Now place the TotalCharges box chart in the second subplot\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.boxplot(x=churn_pd_df['TotalCharges'], color='orange', orient='h')\n",
    "plt.title('Box Plot of TotalCharges')\n",
    "\n",
    "# Adjust layout to prevent overlap and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "WRITE DOWN YOUR ANSWER HERE AS BEFORE.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid them skewing our visualizations and models, we can use a **clamp transformation**, which sets extreme values to predefined upper and lower limits based on the IQR.\n",
    "\n",
    "1. Apply a **clamp transformation** using the `.clip()` method to cap extreme values.\n",
    "2. Replace the modified numeric columns in the main dataset.\n",
    "3. Visualize distributions with boxplots after handling outliers to confirm changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are making modifications to the data, we work on a COPY of\n",
    "# the data until we are happy with the results.\n",
    "churn_clipped_df = churn_df.clone()\n",
    "\n",
    "# TO DO: Apply clamp transformation to handle outliers in churn_clipped_df.\n",
    "# Hint: Clip values based on lower and upper bounds stored in the\n",
    "#   lower_bounds and upper_bounds dictionaries from IQR calculations above.\n",
    "\n",
    "\n",
    "# Show the effects of handling outliers\n",
    "print(\"\\nSummary statistics after handling outliers:\")\n",
    "churn_clipped_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Redo the box plots to visualize distributions after handling\n",
    "# outliers. You can mostly copy code from above, but remember to remake\n",
    "# a pandas version of the Polars DataFrame using churn_clipped_df you\n",
    "# just created.\n",
    "\n",
    "# Adjust layout to prevent overlap and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the remaining outliers in the `MonthlyCharges` boxplot after clamping.  This is actually all of our clamped values for `MonthlyCharges`! When the box plot RECALCULATES the quartiles and median for the clamped data, it is clear these clamped values are still outliers relative to the rest of the data.  This was because our outliers were several times larger than the upper bound and were themselves initially distorting the computed Some people deal with situations like this by iteratively recalculating the IQR and clamping again. \n",
    "\n",
    "However, it turns out a detailed examination of the outliers revealed they were all the result of typographical errors during data entry (e.g., an extra digit added by mistake). Therefore, we will remove these outliers from the dataset instead of clamping them. (We didn't show how to do this in class, so the code to do this is provided, just run it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with outliers in either MonthlyCharges or TotalCharges\n",
    "churn_filtered_df = churn_df.filter(\n",
    "    (pl.col('MonthlyCharges') >= lower_bounds['MonthlyCharges']) &\n",
    "    (pl.col('TotalCharges') >= lower_bounds['TotalCharges']) &\n",
    "    (pl.col('MonthlyCharges') <= upper_bounds['MonthlyCharges']) &\n",
    "    (pl.col('TotalCharges') <= upper_bounds['TotalCharges'])\n",
    ")\n",
    "\n",
    "# Show the statistics of the filtered DataFrame\n",
    "churn_filtered_df.describe()\n",
    "\n",
    "# Make churn_df the filtered DataFrame for further analysis and re-create\n",
    "# the pandas version\n",
    "churn_df = churn_filtered_df\n",
    "churn_pd_df = churn_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plots again\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.boxplot(x=churn_pd_df['MonthlyCharges'], color='skyblue', orient='h')\n",
    "plt.title('Box Plot of MonthlyCharges')\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.boxplot(x=churn_pd_df['TotalCharges'], color='orange', orient='h')\n",
    "plt.title('Box Plot of TotalCharges')\n",
    "\n",
    "# Adjust layout to prevent overlap and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Addressing Missing Values\n",
    "\n",
    "Our Data Quality Plan identified 11 missing values in the `TotalCharges` feature.  Since this is a small percentage of the dataset (0.16%), we can choose to remove these rows without significantly impacting our analysis.  That said, we were warned to only remove entire rows if the missing values are for the target feature. As such, we are going to try imputation first.\n",
    "\n",
    "For continuous features like `TotalCharges`, a common imputation method is to use the mean or median of the feature.  If `TotalCharges` still had some outliers, we would use the median for imputation, as it is more robust to outliers. However, since we have already handled the outliers, we can use the mean for imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Deal with the missing values in TotalCharges by imputing with\n",
    "# the mean In class we showed a case using .full_full() to fill missing\n",
    "# values in an entire dataframe, go ahead and use that if you want.\n",
    "\n",
    "\n",
    "# TO DO: Check to see if there are any missing values left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have the missing values been handled?  How can you tell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "WRITE DOWN YOUR ANSWER HERE AS BEFORE.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Continued Data Exploration and Visualization (Relationships)\n",
    "\n",
    "We addressed some of our data quality issues in Part A.  Now, let's continue our data exploration by examining relationships between features.  This will help us understand how different features interact with each other and with the target variable `Churn`.\n",
    "\n",
    "Recall that we basically have different kinds of plots we use for different kinds of features we are comparing:\n",
    "\n",
    "1. **Continuous vs. Continuous:** Scatter Plots, SPLOM\n",
    "2. **Categorical vs. Categorical:** Small Multiples of Bar Charts or Stacked Bar Charts\n",
    "3. **Categorical vs. Continuous:** Small Multiples of Histograms or Box Plots\n",
    "\n",
    "We will experiment with each of these kinds of plots below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Continuous vs. Continuous\n",
    "\n",
    "We only have three continuous features in this dataset: `Tenure`, `MonthlyCharges`, and `TotalCharges`.  Let's create a scatter plot matrix (SPLOM) to visualize the relationships between these continuous features. (You can find an example of this in the class notes).  Then write down your comments on what you observe in the cell below the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Import Seaborn (sns) and Matplotlib (plt) libraries for \n",
    "# visualization\n",
    "\n",
    "# TO DO: Create a pairplot (scatter plot matrix) using Seaborn, only for\n",
    "# the continuous features (Remember to use the Pandas DataFrame version\n",
    "# we created earlier for compatibility with Seaborn)\n",
    "\n",
    "# Add a title for entire figure and then show it\n",
    "plt.suptitle(\"Scatter Plot Matrix of Continuous Features\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "WRITE DOWN YOUR ANSWER HERE AS BEFORE.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Categorical vs. Categorical\n",
    "\n",
    "We have five categorical features in this dataset (not including `customerID`): `PhoneService` `Contract`, `PaperlessBilling`, `PaymentMethod`, and the target feature `Churn`. In class we reviewed how to visualize relationships between categorial variables using small multiples of histograms or box plots. \n",
    "\n",
    "Let's create small multiples of histograms to visualize the relationships between these categorical features.  Then write down your comments on what you observe in the cell below the plot.  **NOTE**: As you are using Seaborn, remember to use the Pandas DataFrame version of the data, `churn_pd_df`.\n",
    "\n",
    "First Let's examine the relationship between `Contract` and `Churn` using small multiples of bar charts.  Finish the code below to create the bar charts (**HINT**: Most of this is shown in class notes, you just need to fill in the missing pieces).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categorical feature to make bar charts for\n",
    "target_col = 'Churn'\n",
    "# Define the categorical feature to segment by\n",
    "col = 'Contract'\n",
    "categories = churn_df[col].unique().to_list() # Get unique values for feature\n",
    "n_types = len(categories) # Number of unique values\n",
    "\n",
    "# We will make bar charts, one for all data and one for each contract type.\n",
    "# Create a figure with 1 row x n_types columns of subplots\n",
    "fig, axes = plt.subplots(1, n_types+1, figsize=(4*(n_types+1), 4))\n",
    "# Adjust the horizontal and vertical spacing between subplots\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.35)\n",
    "\n",
    "# TO DO: Create a combined box plot of all data in 'axes[0]'\n",
    "\n",
    "\n",
    "\n",
    "# Since we made the 'Churn' a binary feature (0/1), we can manually\n",
    "# set the xticks to show 'No' and 'Yes' instead of 0 and 1\n",
    "axes[0].set_xticks([\"No\", \"Yes\"])\n",
    "\n",
    "# Set the labels and title for the subplot of all data\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(f'All {col} Types')\n",
    "\n",
    "# The following loop goes through each categorical value and creates\n",
    "# a bar chart in its respective subplot (which should be axes[i+1]).\n",
    "for i, category in enumerate(categories):\n",
    "    # Define subset the DataFrame for the current value type\n",
    "    subset = churn_pd_df[churn_pd_df[col] == category]\n",
    "    # TO DO: Create a count plot for the subset in the corresponding subplot\n",
    "\n",
    "\n",
    "    # TO DO: Set title for each subplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice any interesting patterns or trends in the relationship between `Contract` type and customer churn?  Write your observations below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "WRITE DOWN YOUR ANSWER HERE AS BEFORE.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create stacked bar charts to visualize the same relationship.  We're going to go beyond what was shown in class by showing how to make stacked bar charts in terms of both raw counts and proportions.\n",
    "\n",
    "The next three cells don't need to be completed, just run.  However, look at the results of each cell and make sure you understand what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by the categorical feature and target column\n",
    "df_grouped = churn_pd_df.groupby([col, target_col]).size().unstack()\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line divides (.div) our original counts dataframe by the\n",
    "# sum of counts (.sum) in each row. This gives us the proportion of each target\n",
    "# class within each categorical feature value. (NOTE: axis=0 means we are\n",
    "# dividing along rows, i.e., for each category value, whereas the axis=1\n",
    "# in sum(axis=1) means we are summing across columns)\n",
    "df_grouped_prop = df_grouped.div(df_grouped.sum(axis=1), axis=0)\n",
    "df_grouped_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can visualize the same relationship using stacked bar charts both\n",
    "# in terms of counts and proportions.\n",
    "\n",
    "# Create a figure with 1 row x 2 columns of subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "# Create stacked bar chart\n",
    "df_grouped.plot(kind='bar', stacked=True, ax=axes[0])\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(f'Stacked Bar Chart of {target_col} by {col}')\n",
    "\n",
    "# Now make a stacked bar chart to visualize the same relationship but\n",
    "# show the proportions instead of counts.\n",
    "# Create stacked bar chart for proportions\n",
    "df_grouped_prop.plot(kind='bar', stacked=True, ax=axes[1])\n",
    "axes[1].set_ylabel(\"Proportion\")\n",
    "axes[1].set_title(f'Stacked Bar Chart of {target_col} Proportions by {col}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the stacked bar charts produced by the above code and compare them to each other.  They are based on the same data, but visualized differently.  Are the raw counts or the proportions more useful in this case?  Why?  Write your thoughts below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "WRITE DOWN YOUR ANSWER HERE AS BEFORE.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Categorical vs. Continuous\n",
    "\n",
    "We can also check the relationships between categorical and continuous features using small multiples of histograms or box plots, it is really a question of how you want to visualize the distribution of the continuous feature for each category.\n",
    "\n",
    "Let's create small multiples of histograms to visualize the relationship between `Contract` type and `MonthlyCharges`.   The quickest way to do this is to use Seaborn's `FacetGrid` to make a grid of histograms based on the `Contract` type.  \n",
    "\n",
    "The code below \"works\", but recall that `MonthlyCharges` had some high-value outliers, which means the histograms may be misleading.  After running the code, adjust the number of bins used until you get a better\n",
    "sense of the distribution of `MonthlyCharges` for each `Contract` type.  You may also want to change the upper and lower bounds to be something other than the default (which is the entire range of the data). Write your observations below the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, a reminder that Seaborn needs pandas DataFrames churn_pd_df\n",
    "\n",
    "# Set up the plot\n",
    "target_col = 'Contract'\n",
    "seg_col = 'MonthlyCharges'\n",
    "\n",
    "# Create a grid of plots\n",
    "g = sns.FacetGrid(churn_pd_df, col=target_col)\n",
    "\n",
    "# TO DO: Here we execute a matplotlib 'hist' command to get a histogram\n",
    "# in each facet. However, you need the adjust the number of bins and \n",
    "# probably hand-tune the range to get a better sense of the distribution\n",
    "# of MonthlyCharges for each Contract type.\n",
    "n_bins = 6  # Number of bins for histogram\n",
    "lower_bound = churn_pd_df[seg_col].min()\n",
    "upper_bound = churn_pd_df[seg_col].max()\n",
    "\n",
    "# Define the range for the histogram and generate the histograms\n",
    "range_vals = (lower_bound, upper_bound)\n",
    "g.map(plt.hist, seg_col, bins=n_bins, range=range_vals,\n",
    "      edgecolor=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "WRITE DOWN YOUR ANSWER HERE AS BEFORE.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see if the different `Churn` categories have different distributions of `MonthlyCharges` (maybe people churn because they can get a better deal?).  \n",
    "\n",
    "To do this we will use small multiples of box plots to visualize the relationship between `Churn` and `TotalCharges`.   Most of this code is a variant of what we saw in the class notes.  You just need to fill in the missing pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, a reminder that Seaborn needs pandas DataFrames churn_pd_df\n",
    "\n",
    "# Set up the plot\n",
    "target_col = 'Churn'\n",
    "seg_col = 'MonthlyCharges'\n",
    "\n",
    "# Make two plots side by side (using two in 1x3 grid, one subplot will\n",
    "# be 2x wider than the other)\n",
    "fig, _ = plt.subplots(1, 3, figsize=(8,6))\n",
    "plt.subplots_adjust(wspace=0.5)  # Increase horizontal spacing\n",
    "fig.clear()\n",
    "\n",
    "# Create Left axes (1 unit wide) and Right axes (spans 2 units wide)\n",
    "ax1 = plt.subplot2grid((1, 3), (0, 0))\n",
    "ax2 = plt.subplot2grid((1, 3), (0, 1), colspan=2)\n",
    "\n",
    "# Boxplot of Seg_Col values\n",
    "sns.boxplot(y=churn_pd_df[seg_col], ax=ax1, legend=False)\n",
    "ax1.set_title(f\"{seg_col} Distribution\")\n",
    "ax1.set_ylabel(f\"{seg_col}\")\n",
    "# ax1.set_ylim((0, 150)) # Force y-axis limits for better comparison\n",
    "\n",
    "# TO DO: Boxplot of Seg_Col by Target_Col\n",
    "\n",
    "# Label this axis\n",
    "ax2.set_title(f\"{seg_col} Distribution by {target_col}\")\n",
    "ax2.set_title(f\"{seg_col} Distribution\")\n",
    "ax2.set_ylabel(f\"{seg_col}\")\n",
    "# ax2.set_ylim((0, 150)) # Force y-axis limits for better comparison\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you tell what the relationship is between `MonthlyCharges` and `Churn` from these box plots?  Write your observations below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "WRITE DOWN YOUR ANSWER HERE AS BEFORE.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgUeUoGAQaM_"
   },
   "source": [
    "## Part C: Feature Engineering\n",
    "\n",
    "Feature engineering normally refers to the process of creating new features from existing ones to improve model performance. \n",
    "\n",
    "In this case, we will create a new feature called `AvgMonthlyCharges`, which is calculated by dividing `TotalCharges` by `Tenure`. This feature represents the average monthly charges for each customer over their tenure with the company. We have reason to suspect that customers with higher average monthly charges may be more likely to churn, as they may perceive the service as being too expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: We can easily compute the average monthly charges by dividing\n",
    "# TotalCharges by tenure.  We will create a new feature called\n",
    "# AvgMonthlyCharges by dividing pl.col('TotalCharges') by\n",
    "# pl.col('tenure') and assigning it to alias `AvgMonthlyCharges`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# See the first few rows to verify the new feature\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore if a user is paying more now than their average monthly charge, they may be more likely to churn.  We can create a categorial feature called `MonthlyChargeTrend` that is either `rising`, `falling`, or `stable` based on whether their current `MonthlyCharges` is above, below, or equal to their `AvgMonthlyCharges`.  This feature could help us identify customers who are experiencing changes in their billing that may influence their decision to churn.\n",
    "\n",
    "**BIG HINT**: You can do this using the `.with_columns()` method along with conditional expressions using `.when()`, `.then()`, and `.otherwise()` methods as shown in class.  Since we have 3 conditions to check, you will need to chain multiple `.when()` statements together. To have `.alias()` apply to the entire conditional expression, you will need to wrap the entire expression in parentheses.  Also, to set values to strings, you need to use `pl.lit(\"string_value\")` (`pl.lit` defines a \"literal\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Let's create the categorical feature MonthyChargeTrend based on\n",
    "# whether the AvgMonthlyCharges is above, below, or equal to the overall\n",
    "# average.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the pandas version again for visualization with Seaborn\n",
    "churn_pd_df = churn_df.to_pandas()\n",
    "\n",
    "# Show the first few rows to verify the new feature\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to see visually if either of these new features have a relationship with `Churn` by making the appropriate plots.  Write your observations below the plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATE CODE CELLS WITH YOUR PLOTS AND ADD MARKDOWN CELLS WITH YOUR OBSERVATIONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D: Normalization and Scaling\n",
    "\n",
    "Once we have all our features ready, we may need to normalize or scale them before feeding them into a machine learning model. This is especially important for algorithms that are sensitive to the scale of input features, such as k-nearest neighbors (KNN) and support vector machines (SVM).\n",
    "\n",
    "For this dataset, we can apply Min-Max Scaling to the continuous features `Tenure`, `MonthlyCharges`, `TotalCharges`, and `AvgMonthlyCharges`. This technique scales the features to a fixed range, typically [0, 1], which helps in normalizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of a list of the features to be normalized/scaled\n",
    "continuous_features = ['tenure', 'MonthlyCharges', 'TotalCharges', \n",
    "                       'AvgMonthlyCharges']\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Extract columns to be scaled\n",
    "features_to_scale = churn_df.select(continuous_features)\n",
    "\n",
    "# Create scaled_data numpy array as the scaled version of the features\n",
    "scaled_data = scaler.fit_transform(features_to_scale)\n",
    "\n",
    "# Add normalized features back to the DataFrame with new column names\n",
    "for i, col in enumerate(continuous_features):\n",
    "    churn_df = churn_df.with_columns(\n",
    "        pl.Series(f\"{col}_norm\", scaled_data[:, i])\n",
    "    )\n",
    "\n",
    "# Show the first few rows to verify the new normalized features\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out AvgMonthlyCharges has some pretty extreme large-value outliers, why is this a problem when using Min-Max Scaling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "WRITE DOWN YOUR ANSWER HERE AS BEFORE.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might consider standardization (z-score normalization) as an alternative scaling method, which is less sensitive to outliers. However, given that we don't see a particularly normal distribution for `AvgMonthlyCharges`, standardization may not be the best choice either.  We will hold off on doing any more for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed, I tended to create new columns when normalizing or binning features rather than replacing the original columns.  Why might this be a good idea?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "WRITE DOWN YOUR ANSWER HERE AS BEFORE.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZc2NejlR7j4"
   },
   "source": [
    "## What you need to submit\n",
    "\n",
    "Once you have completed this lab, you need to submit your work. You should submit the `.ipynb` notebook file. To do this, go to the File menu and select **Download > Download .ipynb**(this is the native Jupyter notebook format).  Submit that file to the Lab 4 dropbox on D2L."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMvHcvJ1qivH7TxKdHBG7XT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
